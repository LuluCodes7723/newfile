# -*- coding: utf-8 -*-
"""NLP Practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1weph6y0-22v0t0iBi-5eoeTtQbdUY8LT
"""

#Tokenization

pip install nltk

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab') # pre tained tokenizer model


text = "Barack Obama visited India in 2015 to strengthen the relationship between the two countries."
tokens = word_tokenize(text)

print(tokens)

#Remove Stop Words

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

nltk.download('stopwords')


text = "Barack Obama visited India in 2015 to strengthen the relationship between the two countries."
words = word_tokenize(text) # tokenization


# print (stopwords.words('english'))    ----    shows all the stop words in English
filtered=[]
for w in words:
    if w not in stopwords.words('english'):
        filtered.append(w)
print(filtered)

#Lemmetization

import nltk
nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
words = filtered

lemmas = [lemmatizer.lemmatize(w) for w in words]    #general lemmatization == reducing to singular form
lemmas1 = [lemmatizer.lemmatize(w,pos='a') for w in words] #specificaly adjective lemmatization
lemmas2 = [lemmatizer.lemmatize(w,pos='n') for w in words] #specificaly noun lemmatization
lemmas3 = [lemmatizer.lemmatize(w,pos='r') for w in words] #specificaly adverb lemmatization
lemmas4 = [lemmatizer.lemmatize(w,pos='v') for w in words] #specificaly verb lemmatization





print("General Lemmatization: ",lemmas)
print("Adjective Lemmatization: ",lemmas1)
print("Noun Lemmatization: ",lemmas2)
print("Adverb Lemmatization: ",lemmas3)
print("Verb Lemmatization: ",lemmas4)

#POS (Parts of Speech) Tagging

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')  #nltk.download('averaged_perceptron_tagger')

tokens = lemmas

pos_tags = nltk.pos_tag(tokens)
print(pos_tags)

#Name Entity Recognizition

#pip install spacy
#python -m spacy download en_core_web_sm     ---- would work in Jupyter notebook, but not in colab

import spacy

nlp = spacy.load("en_core_web_sm")

text = "Barack Obama visited India in 2015 to strengthen the relationship between the two countries."

#text = " ".join(lemmas) #this command combines all the elements of a list to create a string ex. it is joining all the tokens/elements of the lemmas to create the original sentence
                       #===== "Barack Obama visited India in 2015 to strengthen the relationship between the two countries."
#we noticed that the "2015 == DATE" NER was not displaying so we put the string back with the entire sentence

doc = nlp(text)

for ent in doc.ents:
    print(ent.text, "->", ent.label_)

print(text)